from pathlib import PurePath

import streamlit as st
import torch
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import traceback
from torch.cuda.amp import autocast, GradScaler
import gc

from db.models import Comment

MODEL_CHECKPOINT = "DeepPavlov/rubert-base-cased"
import os
# –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞ –∏ —Å—Ç—Ä–æ–∏–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª—è–º
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))  # –ü–æ–¥–Ω–∏–º–∞–µ–º—Å—è –Ω–∞ –¥–≤–∞ —É—Ä–æ–≤–Ω—è –≤–≤–µ—Ä—Ö
MODEL_TONE_PATH = os.path.join(project_root, "models", "model_tone.pth")
MODEL_CLASS_PATH = os.path.join(project_root, "models", "model_class.pth")

# –ú–∞–ø–ø–∏–Ω–≥ –¥–ª—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π
TONE_MAPPING = {
    0: "–û—Å–∫–æ—Ä–±–ª–µ–Ω–∏–µ",
    1: "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 
    2: "–ü–æ–∑–∏—Ç–∏–≤–Ω–æ–µ"
}

# –ú–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–µ–Ω–∞–≤–∏—Å—Ç–∏
HATE_MAPPING = {
    0: "–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –æ—Å–∫–∞—Ä–±–ª–µ–Ω–∏—è",
    1: "–ö—Å–µ–Ω–æ—Ñ–æ–±–∏—è",
    2: "–ì–æ–º–æ—Ñ–æ–±–∏—è", 
    3: "C–µ–∫—Å–∏–∑–º",
    4: "–õ—É–∫–∏–∑–º",
    5: "–î—Ä—É–≥–æ–µ"
}

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
if torch.cuda.is_available():
    DEVICE = torch.device("cuda")
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # –≤ GB
    st.success(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {gpu_name} ({gpu_memory:.1f} GB)")
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è GPU
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–∞–º—è—Ç–∏ GPU
    if gpu_memory >= 8:
        BATCH_SIZE = 16
    elif gpu_memory >= 4:
        BATCH_SIZE = 8
    else:
        BATCH_SIZE = 4
else:
    DEVICE = torch.device("cpu")
    BATCH_SIZE = 4
    st.warning("‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞! –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU. –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:")
    st.markdown("""
    - –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å CUDA Toolkit
    - –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA: `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118`
    - –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç CUDA
    """)
    st.info("üí° –¢–µ–∫—É—â–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∏–∂–µ –æ–∂–∏–¥–∞–µ–º–æ–π")

# –í–∫–ª—é—á–µ–Ω–∏–µ mixed precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
USE_AMP = torch.cuda.is_available()


@st.cache_resource(show_spinner=False)
def load_tokenizer():
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)
        return tokenizer
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        raise


@st.cache_resource(show_spinner=False)
def load_model_tone():
    try:
        model_tone = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=3)
        model_tone.load_state_dict(torch.load(MODEL_TONE_PATH, map_location=DEVICE))
        model_tone.to(DEVICE)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è GPU
        if torch.cuda.is_available():
            model_tone = model_tone.half()  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ float16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        
        model_tone.eval()
        return model_tone
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
        raise


@st.cache_resource(show_spinner=False)
def load_model_class():
    try:
        model_class = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=6)
        model_class.load_state_dict(torch.load(MODEL_CLASS_PATH, map_location=DEVICE))
        model_class.to(DEVICE)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è GPU
        if torch.cuda.is_available():
            model_class = model_class.half()  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ float16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        
        model_class.eval()
        return model_class
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
        raise


# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
try:
    tokenizer = load_tokenizer()
    model_tone = load_model_tone()
    model_class = load_model_class()
except Exception as e:
    st.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π: {e}")
    st.error("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–µ–π –≤ –ø–∞–ø–∫–µ models/")
    raise


def clear_gpu_memory():
    """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ GPU"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()


def predict(data):
    try:
        df_tone = data.copy()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ sentence
        if 'sentence' not in df_tone.columns:
            raise ValueError("–í –¥–∞–Ω–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'sentence'")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—É—Å—Ç—ã–µ
        if df_tone.empty:
            raise ValueError("–ü–æ–ª—É—á–µ–Ω—ã –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

        # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        df_tone = df_tone.dropna(subset=['sentence'])
        df_tone = df_tone[df_tone['sentence'].str.strip() != '']

        if df_tone.empty:
            raise ValueError("–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        device_info = "GPU" if torch.cuda.is_available() else "CPU"
        st.info(f"‚ö° –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(df_tone)} –∑–∞–ø–∏—Å–µ–π –Ω–∞ {device_info} —Å batch_size={BATCH_SIZE}...")

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è GPU
        tokenized_data = tokenizer(
            df_tone["sentence"].tolist(), 
            padding=True, 
            truncation=True, 
            max_length=512,
            return_tensors="pt"
        )

        # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ GPU
        if torch.cuda.is_available():
            tokenized_data = {k: v.to(DEVICE) for k, v in tokenized_data.items()}

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º mixed precision
        predictions_tone = []
        predictions_class = []

        model_tone.eval()
        model_class.eval()

        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ autocast –¥–ª—è mixed precision
        with torch.no_grad():
            with autocast(enabled=USE_AMP):
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                total_batches = (len(df_tone) + BATCH_SIZE - 1) // BATCH_SIZE
                
                # –°–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                for i in range(0, len(df_tone), BATCH_SIZE):
                    batch_end = min(i + BATCH_SIZE, len(df_tone))
                    
                    # –°–æ–∑–¥–∞–Ω–∏–µ –±–∞—Ç—á–∞
                    batch_data = {
                        'input_ids': tokenized_data['input_ids'][i:batch_end],
                        'attention_mask': tokenized_data['attention_mask'][i:batch_end]
                    }
                    
                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
                    outputs_tone = model_tone(**batch_data)
                    logits_tone = outputs_tone.logits
                    predictions_tone.extend(torch.argmax(logits_tone, dim=-1).cpu().numpy())

                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞
                    outputs_class = model_class(**batch_data)
                    logits_class = outputs_class.logits
                    predictions_class.extend(torch.argmax(logits_class, dim=-1).cpu().numpy())

                    # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
                    if torch.cuda.is_available():
                        del outputs_tone, outputs_class, logits_tone, logits_class
                        torch.cuda.empty_cache()

                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
                    if total_batches > 1:
                        current_batch = (i // BATCH_SIZE) + 1
                        progress = current_batch / total_batches
                        progress_bar.progress(progress)
                        status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –±–∞—Ç—á–µ–π: {current_batch}/{total_batches}")

        # –û—á–∏—â–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        progress_bar.empty()
        status_text.empty()

        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ GPU
        clear_gpu_memory()

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        df_tone["tone_prediction"] = predictions_tone
        df_tone["class_prediction"] = predictions_class

        # –ò–∑–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∫–ª–∞—Å—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Ç–æ–Ω–∞
        for i in range(len(df_tone)):
            if df_tone.loc[i, "tone_prediction"] in [1, 2]:
                df_tone.loc[i, "class_prediction"] = 0
            elif df_tone.loc[i, "tone_prediction"] == 0 and df_tone.loc[i, "class_prediction"] == 0:
                df_tone.loc[i, "class_prediction"] = 5

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è–º–∏
        df_tone["tone_name"] = df_tone["tone_prediction"].map(TONE_MAPPING)
        df_tone["hate_name"] = df_tone["class_prediction"].map(HATE_MAPPING)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        try:
            for idx, comment in df_tone.iterrows():
                Comment.create(
                    text=comment["sentence"],
                    tone_id=comment["tone_prediction"] + 1,
                    hate_id=comment["class_prediction"] + 1
                )
        except Exception as e:
            st.warning(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö: {e}")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
        if torch.cuda.is_available():
            st.success(f"üöÄ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(df_tone)} –∑–∞–ø–∏—Å–µ–π –Ω–∞ GPU.")
        else:
            st.success(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(df_tone)} –∑–∞–ø–∏—Å–µ–π –Ω–∞ CPU.")
            st.info("üí° –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å GPU")
        
        return df_tone

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {str(e)}")
        st.error("–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –æ—à–∏–±–∫–∏:")
        st.code(traceback.format_exc())
        raise
